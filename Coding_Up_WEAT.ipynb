{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coding-Up-WEAT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chetan-parthiban/Conceptors/blob/master/Coding_Up_WEAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTJbFCEkkIyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from itertools import combinations, filterfalse\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgm09syZzCTH",
        "colab_type": "text"
      },
      "source": [
        "# Preparing WEAT Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIn-028wmiy2",
        "colab_type": "text"
      },
      "source": [
        "**Cosine Similarity** <br>\n",
        "Define functions pertinent to computing cosine similarity/distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U_u9OaslpiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def similarity_difference(W, A, B):\n",
        "    \n",
        "    simA = cosine_similarity(W,A)\n",
        "    simB = cosine_similarity(W,B)\n",
        "    \n",
        "    meanA = np.mean(simA, axis = 1)\n",
        "    meanB = np.mean(simB, axis = 1)\n",
        "    \n",
        "    return meanA - meanB\n",
        "\n",
        "def association_difference(W, Z,A,B):  \n",
        "    swAB = similarity_difference(W,A,B)\n",
        "    szAB = similarity_difference(Z,A,B)\n",
        "    return sum(swAB) - sum(szAB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRKJxHWu2dTz",
        "colab_type": "text"
      },
      "source": [
        "**Calculating p-value**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7QCwI9hn1lI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_permutation(iterable, r=None):\n",
        "    pool = tuple(iterable)\n",
        "    r = len(pool) if r is None else r\n",
        "    return tuple(random.sample(pool, r))\n",
        "\n",
        "\n",
        "def WEAT_pval(W,Z,A,B,test_statistic, sample):\n",
        "    \n",
        "    perm_size = W.shape[0]\n",
        "    WuZ = np.vstack((W,Z))\n",
        "    distribution = []\n",
        "    \n",
        "    if not sample:\n",
        "        permutations = combinations(WuZ, perm_size)\n",
        "    else:\n",
        "        permutations = [random_permutation(WuZ, perm_size) for s in range(sample)]\n",
        "    \n",
        "    for Wi in permutations:\n",
        "        Zi = []\n",
        "        def ifWi(w):\n",
        "            for wi in Wi:\n",
        "                if np.all(w == wi):\n",
        "                    return True\n",
        "            return False\n",
        "\n",
        "        Zi = filterfalse(ifWi, WuZ)\n",
        "        Zi = [z for z in Zi]    \n",
        "        distribution.append(association_difference(Wi,Zi,A,B))\n",
        "\n",
        "    greaterthan = np.array([o > test_statistic for o in distribution])\n",
        "    \n",
        "    return greaterthan.sum()/greaterthan.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ispdd7Rn3NO",
        "colab_type": "text"
      },
      "source": [
        "**WEAT Final Form** <br>\n",
        "Final function - takes in two sets of target words and two sets of attribute words and performs the word embedding association test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn23sjWpn1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def WEAT(W, Z, A, B, sample = 2000):\n",
        "\n",
        "    test_statistic = association_difference(W,Z,A,B)\n",
        "    p = WEAT_pval(W,Z,A,B,test_statistic, sample)\n",
        "    \n",
        "    std = np.sqrt(np.var(similarity_difference(np.vstack((W,Z)), A, B)))   \n",
        "    effect_size = test_statistic/(std*W.shape[0])\n",
        "   \n",
        "    return effect_size, p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBr4frwJzHjr",
        "colab_type": "text"
      },
      "source": [
        "# Importing Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDIdbwdvzOBl",
        "colab_type": "code",
        "outputId": "6a2e0a75-9284-41f0-ba04-88f196e3bcaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1139
        }
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/54/76374f9a448ca765446502e7f2bb53c976e9c055102290fe6f8b0b038b37/flair-0.4.1.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.1.0)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
            "Collecting segtok>=1.5.7 (from flair)\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.0.3)\n",
            "Collecting mpld3>=0.3 (from flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n",
            "Collecting sqlitedict>=1.6.0 (from flair)\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Collecting deprecated>=1.2.4 (from flair)\n",
            "  Downloading https://files.pythonhosted.org/packages/9f/7a/003fa432f1e45625626549726c2fbb7a29baa764e9d1fdb2323a5d779f8a/Deprecated-1.2.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Collecting pytorch-pretrained-bert>=0.6.1 (from flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 41.2MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.2.9 (from flair)\n",
            "  Downloading https://files.pythonhosted.org/packages/57/90/8760eaa97c5a2f676f3f350fd43e79f8d9e4f9c42362c62f733e81e37d33/bpemb-0.2.12-py3-none-any.whl\n",
            "Requirement already satisfied: regex==2018.1.10 in /usr/local/lib/python3.6/dist-packages (from flair) (2018.1.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->flair) (1.16.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.8.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.21.1)\n",
            "Requirement already satisfied: wrapt<2,>=1 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.10.11)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.8.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.1->flair) (1.9.150)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.1->flair) (2.21.0)\n",
            "Collecting sentencepiece (from bpemb>=0.2.9->flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/95/7f357995d5eb1131aa2092096dca14a6fc1b1d2860bd99c22a612e1d1019/sentencepiece-0.1.82-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 40.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.2.3->flair) (41.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->flair) (0.12.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.150 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (1.12.150)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (1.24.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.150->boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.14)\n",
            "Building wheels for collected packages: flair, segtok, mpld3, sqlitedict\n",
            "  Building wheel for flair (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/55/6b/c12cf58209b8346f653a04f37dd8f607ab0e85a26238a23420\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "Successfully built flair segtok mpld3 sqlitedict\n",
            "Installing collected packages: segtok, mpld3, sqlitedict, deprecated, pytorch-pretrained-bert, sentencepiece, bpemb, flair\n",
            "Successfully installed bpemb-0.2.12 deprecated-1.2.5 flair-0.4.1 mpld3-0.3 pytorch-pretrained-bert-0.6.2 segtok-1.5.7 sentencepiece-0.1.82 sqlitedict-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bli8Zet7zYvf",
        "colab_type": "code",
        "outputId": "7c1df79a-233a-4546-954e-a46476767598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "from flair.embeddings import WordEmbeddings\n",
        "from flair.embeddings import ELMoEmbeddings\n",
        "from flair.data import Sentence\n",
        "glove = WordEmbeddings('glove')\n",
        "fast = WordEmbeddings('crawl')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-22 23:08:50,213 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmp_b6u7pdm\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 160000128/160000128 [00:08<00:00, 19756692.80B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-22 23:08:58,863 copying /tmp/tmp_b6u7pdm to cache at /root/.flair/embeddings/glove.gensim.vectors.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-22 23:08:59,308 removing temp file /tmp/tmp_b6u7pdm\n",
            "2019-05-22 23:08:59,820 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim not found in cache, downloading to /tmp/tmp_1cj3rgb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21494764/21494764 [00:01<00:00, 12022193.96B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-22 23:09:02,141 copying /tmp/tmp_1cj3rgb to cache at /root/.flair/embeddings/glove.gensim\n",
            "2019-05-22 23:09:02,181 removing temp file /tmp/tmp_1cj3rgb\n",
            "2019-05-22 23:09:02,183 this function is deprecated, use smart_open.open instead\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-22 23:09:04,382 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/en-fasttext-crawl-300d-1M.vectors.npy not found in cache, downloading to /tmp/tmp0k8wtypz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200000128/1200000128 [00:56<00:00, 21322455.20B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-22 23:10:01,218 copying /tmp/tmp0k8wtypz to cache at /root/.flair/embeddings/en-fasttext-crawl-300d-1M.vectors.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-22 23:10:08,137 removing temp file /tmp/tmp0k8wtypz\n",
            "2019-05-22 23:10:08,617 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/en-fasttext-crawl-300d-1M not found in cache, downloading to /tmp/tmp2ug9smfw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39323680/39323680 [00:02<00:00, 14700195.06B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-22 23:10:11,879 copying /tmp/tmp2ug9smfw to cache at /root/.flair/embeddings/en-fasttext-crawl-300d-1M\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-22 23:10:11,945 removing temp file /tmp/tmp2ug9smfw\n",
            "2019-05-22 23:10:11,946 this function is deprecated, use smart_open.open instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CQUOrqnzlKy",
        "colab_type": "text"
      },
      "source": [
        "# Obtaining Brown Corpus and WordLists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NvuDPgWzqiD",
        "colab_type": "code",
        "outputId": "aed3b9e3-58fe-4dea-ef88-d1e7ec55b2e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "t = np.transpose\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "\n",
        "from nltk.corpus import brown\n",
        "brown_corpus = brown.sents()\n",
        "brown_corpus = brown_corpus[:10000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt375chRz8s4",
        "colab_type": "code",
        "outputId": "5e7687fd-59dd-4ef0-d1ba-ef8b767773b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "# Gender word lists\n",
        "!git clone https://github.com/uclanlp/gn_glove\n",
        "!git clone https://github.com/uclanlp/corefBias\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/female.txt\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/male.txt\n",
        "    \n",
        "# our code for debiasing -- also includes word lists    \n",
        "!git clone https://github.com/jsedoc/ConceptorDebias"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gn_glove'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 162 (delta 18), reused 25 (delta 9), pack-reused 114\u001b[K\n",
            "Receiving objects: 100% (162/162), 73.36 KiB | 5.24 MiB/s, done.\n",
            "Resolving deltas: 100% (64/64), done.\n",
            "Cloning into 'corefBias'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 471 (delta 3), reused 0 (delta 0), pack-reused 457\u001b[K\n",
            "Receiving objects: 100% (471/471), 84.18 MiB | 37.77 MiB/s, done.\n",
            "Resolving deltas: 100% (273/273), done.\n",
            "--2019-05-22 23:11:37--  https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/female.txt\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35751 (35K) [text/plain]\n",
            "Saving to: ‘female.txt’\n",
            "\n",
            "female.txt          100%[===================>]  34.91K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2019-05-22 23:11:37 (590 KB/s) - ‘female.txt’ saved [35751/35751]\n",
            "\n",
            "--2019-05-22 23:11:38--  https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/male.txt\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20466 (20K) [text/plain]\n",
            "Saving to: ‘male.txt’\n",
            "\n",
            "male.txt            100%[===================>]  19.99K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-05-22 23:11:38 (163 MB/s) - ‘male.txt’ saved [20466/20466]\n",
            "\n",
            "Cloning into 'ConceptorDebias'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 410 (delta 53), reused 12 (delta 6), pack-reused 308\u001b[K\n",
            "Receiving objects: 100% (410/410), 3.73 MiB | 18.61 MiB/s, done.\n",
            "Resolving deltas: 100% (219/219), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKiwwA590HHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Winobias word lists\n",
        "winoWordsPath = './' + 'corefBias/WinoBias/wino/extra_gendered_words.txt'\n",
        "male_vino_extra = []\n",
        "female_vino_extra = []\n",
        "with open(winoWordsPath, \"r+\") as f_in:\n",
        "    for line in f_in:\n",
        "        male_vino_extra.append(line.split('\\t')[0])\n",
        "        female_vino_extra.append(line.strip().split('\\t')[1])\n",
        "\n",
        "# Get CMU word lists\n",
        "cmuMaleWordPath = './' + 'male.txt'\n",
        "male_cmu = []\n",
        "with open(cmuMaleWordPath, \"r+\") as f_in:\n",
        "  for line in f_in:\n",
        "    w = line.strip()\n",
        "    if len(w)>0 and w[0] != '#':\n",
        "      male_cmu.append(w)\n",
        "cmuFemaleWordPath = './' + 'female.txt'\n",
        "female_cmu = []\n",
        "with open(cmuFemaleWordPath, \"r+\") as f_in:\n",
        "  for line in f_in:\n",
        "    w = line.strip()\n",
        "    if len(w)>0 and w[0] != '#':\n",
        "      female_cmu.append(w)\n",
        "    \n",
        "# Get gnGlove word lists\n",
        "gnGloveFemaleWordPath = './' + 'gn_glove/wordlist/female_word_file.txt'\n",
        "female_gnGlove = []\n",
        "with open(gnGloveFemaleWordPath, \"r+\") as f_in:\n",
        "    for line in f_in:\n",
        "        female_gnGlove.append(line.strip())\n",
        "gnGloveMaleWordPath = './' + 'gn_glove/wordlist/male_word_file.txt'\n",
        "male_gnGlove = []\n",
        "with open(gnGloveMaleWordPath, \"r+\") as f_in:\n",
        "    for line in f_in:\n",
        "        male_gnGlove.append(line.strip())\n",
        "    \n",
        "# Get WEAT lists and conceptor functionality\n",
        "from ConceptorDebias.Conceptors.conceptor_fxns import *\n",
        "from ConceptorDebias.lists import WEAT_lists\n",
        "WEATLists = WEAT_lists.WEATLists()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yX-3QO10UII",
        "colab_type": "text"
      },
      "source": [
        "# Adding Some Useful Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psy0f2pm0QW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pick_embeddings(embedding, word_list):\n",
        "    labels = []\n",
        "    sentence = Sentence(' '.join(word_list))\n",
        "    embedding.embed(sentence)\n",
        "    X = torch.stack([token.embedding for token in sentence]).numpy()\n",
        "    for w in sentence:\n",
        "        labels.append(w)\n",
        "    return X, labels\n",
        "\n",
        "def do_plot(X_fit, title=None, labels = ['']):\n",
        "    dimension = X_fit.shape[1]\n",
        "    label_types = sorted(list(set(labels)))\n",
        "    num_labels = len(label_types)\n",
        "    colors = cm.Accent(np.linspace(0,1,num_labels))\n",
        "    with plt.style.context(plt_style):\n",
        "        fig = plt.figure()\n",
        "        if dimension == 2:\n",
        "            ax = fig.add_subplot(111)\n",
        "            \n",
        "            for lab,col in zip(label_types, colors):\n",
        "                if num_labels>1:\n",
        "                    idxs = [i for i,v in enumerate(labels) if v == lab]\n",
        "                    ax.scatter([X_fit[i,0] for i in idxs], \n",
        "                               [X_fit[i,1] for i in idxs], \n",
        "                               c = [col], label = lab)\n",
        "                else:\n",
        "                    ax.scatter(X_fit[:,0],\n",
        "                               X_fit[:,1],\n",
        "                               c = [col])\n",
        "        elif dimension == 3:\n",
        "            ax = fig.add_subplot(111, projection ='3d')\n",
        "            for lab, col in zip(label_types,colors):\n",
        "                ax.scatter(X_fit[labels==lab,0],\n",
        "                           X_fit[labels==lab,1],\n",
        "                           X_fit[labels==lab,2],\n",
        "                           c=[col])\n",
        "        else:\n",
        "            raise Exception('Bad Dimensions')\n",
        "        plt.title(title)\n",
        "        if num_labels >1:\n",
        "            ax.legend()\n",
        "        plt.show()\n",
        "        \n",
        "gender_list_pronouns = WEATLists.W_7_Male_terms + WEATLists.W_7_Female_terms + WEATLists.W_8_Male_terms + WEATLists.W_8_Female_terms\n",
        "gender_list_pronouns = list(set(gender_list_pronouns))\n",
        "\n",
        "pronouns_male = WEATLists.W_7_Male_terms + WEATLists.W_8_Male_terms\n",
        "pronouns_male = list(set(pronouns_male))\n",
        "pronouns_female = WEATLists.W_8_Female_terms + WEATLists.W_8_Female_terms\n",
        "pronouns_female = list(set(pronouns_female))\n",
        "\n",
        "\n",
        "gender_list_extended = male_vino_extra + female_vino_extra + male_gnGlove + female_gnGlove\n",
        "gender_list_extended = list(set(gender_list_extended))\n",
        "\n",
        "extended_male = male_vino_extra + male_gnGlove\n",
        "extended_male = list(set(extended_male))\n",
        "extended_female = female_vino_extra + female_gnGlove\n",
        "extended_female = list(set(extended_female))\n",
        "\n",
        "gender_list_propernouns = male_cmu + female_cmu\n",
        "gender_list_propernouns = list(set(gender_list_propernouns))\n",
        "\n",
        "propernouns_male = list(set(male_cmu))\n",
        "propernouns_female = list(set(female_cmu))\n",
        "\n",
        "gender_list_all = gender_list_pronouns + gender_list_extended + gender_list_propernouns\n",
        "gender_list_all = list(set(gender_list_all))\n",
        "\n",
        "all_male = list(set(pronouns_male + extended_male + propernouns_male))\n",
        "all_female = list(set(pronouns_female + extended_female + propernouns_female))\n",
        "\n",
        "career = list(set(WEATLists.W_6_Career))\n",
        "family = list(set(WEATLists.W_6_Family))\n",
        "malename = list(set(WEATLists.W_6_Male_names))\n",
        "femalename = list(set(WEATLists.W_6_Female_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHnf6Bl00mnU",
        "colab_type": "text"
      },
      "source": [
        "# Testing WEAT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ75eIOR0qHE",
        "colab_type": "code",
        "outputId": "aaec1318-c7d6-4ec1-b484-4db8682aef2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pronouns_male = ['him', 'his', 'uncle', 'grandfather', 'son', 'he', 'boy', 'father', 'brother']\n",
        "pronouns_female = ['her', 'grandmother', 'aunt', 'hers', 'daughter', 'sister', 'mother', 'she', 'girl']\n",
        "emb = fast\n",
        "\n",
        "male_e, _ = pick_embeddings(fast,pronouns_male)\n",
        "female_e, _ = pick_embeddings(fast,pronouns_female)\n",
        "career_e, _ = pick_embeddings(fast,career)\n",
        "family_e, _ = pick_embeddings(fast,family)\n",
        "\n",
        "effect_size, p = WEAT(male_e, female_e, career_e, family_e, sample = 10000)\n",
        "\n",
        "print('Effect Size: ', effect_size, '\\n', 'p-value: ', p)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Effect Size:  0.399042785590551 \n",
            " p-value:  0.2105\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}